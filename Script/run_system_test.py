"""
ì¬ë¬´ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ - ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í†µí•© í…ŒìŠ¤íŠ¸
ì‹¤ì œ ì›ì¥ íŒŒì¼ì„ ì‚¬ìš©í•œ ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import json
import traceback
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

# ë©”ì¸ ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from main_processor import MainProcessor
    from logging_system import UTF8LoggingSystem
except ImportError as e:
    print(f"ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜: {e}")
    print("í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:", os.getcwd())
    print("ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬:", script_dir)
    sys.exit(1)

class SystemIntegrationTest:
    """ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.test_session_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.test_results = {
            'session_id': self.test_session_id,
            'start_time': datetime.now().isoformat(),
            'tests_performed': [],
            'overall_success': False,
            'error_summary': [],
            'performance_metrics': {}
        }
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (Scriptì˜ ìƒìœ„ ë””ë ‰í† ë¦¬)
        self.project_root = script_dir.parent
        self.ledger_dir = self.project_root / "Ledgers"
        
        print(f"=== ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        print(f"í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ID: {self.test_session_id}")
        print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {self.project_root}")
        print(f"ì›ì¥ ë””ë ‰í† ë¦¬: {self.ledger_dir}")
        print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
    
    def discover_test_files(self):
        """í…ŒìŠ¤íŠ¸ìš© ì›ì¥ íŒŒì¼ ë°œê²¬"""
        if not self.ledger_dir.exists():
            print(f"âŒ ì›ì¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.ledger_dir}")
            return []
        
        # Excel íŒŒì¼ ê²€ìƒ‰
        excel_files = list(self.ledger_dir.glob("*.xlsx"))
        
        # ì„ì‹œ íŒŒì¼ ì œì™¸
        test_files = [f for f in excel_files if not f.name.startswith('~$')]
        
        print(f"ë°œê²¬ëœ í…ŒìŠ¤íŠ¸ íŒŒì¼ ({len(test_files)}ê°œ):")
        for i, file_path in enumerate(test_files, 1):
            file_size = file_path.stat().st_size / (1024 * 1024)  # MB
            print(f"  {i}. {file_path.name} ({file_size:.1f} MB)")
        
        return test_files
    
    def run_basic_system_test(self):
        """ê¸°ë³¸ ì‹œìŠ¤í…œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("--- ê¸°ë³¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ---")
        
        test_result = {
            'test_name': 'basic_system_test',
            'start_time': datetime.now().isoformat(),
            'success': False,
            'details': {}
        }
        
        try:
            # ë©”ì¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
            print("1. ë©”ì¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”...")
            processor = MainProcessor()
            print("âœ… ë©”ì¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì„±ê³µ")
            
            # ì„¤ì • ë¡œë“œ í…ŒìŠ¤íŠ¸
            print("2. ì„¤ì • ê²€ì¦...")
            config = processor.config
            required_sections = ['processing', 'validation', 'output']
            
            for section in required_sections:
                if section not in config:
                    raise Exception(f"í•„ìˆ˜ ì„¤ì • ì„¹ì…˜ ëˆ„ë½: {section}")
            
            print("âœ… ì„¤ì • ê²€ì¦ ì™„ë£Œ")
            
            # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
            print("3. ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ê²€ì¦...")
            components = {
                'logger': processor.logger,
                'validator': processor.validator,
                'marker': processor.marker,
                'mcp': processor.mcp,
                'batch_processor': processor.batch_processor
            }
            
            for comp_name, comp_instance in components.items():
                if comp_instance is None:
                    raise Exception(f"ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {comp_name}")
            
            print("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì •ìƒ ì´ˆê¸°í™”")
            
            test_result['success'] = True
            test_result['details'] = {
                'components_initialized': list(components.keys()),
                'config_sections': list(config.keys())
            }
            
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            test_result['details']['error'] = str(e)
            test_result['details']['traceback'] = traceback.format_exc()
        
        test_result['end_time'] = datetime.now().isoformat()
        test_result['duration'] = (datetime.fromisoformat(test_result['end_time']) - 
                                  datetime.fromisoformat(test_result['start_time'])).total_seconds()
        
        self.test_results['tests_performed'].append(test_result)
        return test_result['success']
    
    def run_file_processing_test(self, test_file_path: Path):
        """ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print(f"--- íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸: {test_file_path.name} ---")
        
        test_result = {
            'test_name': 'file_processing_test',
            'file_name': test_file_path.name,
            'file_size_mb': test_file_path.stat().st_size / (1024 * 1024),
            'start_time': datetime.now().isoformat(),
            'success': False,
            'details': {}
        }
        
        try:
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            output_dir = self.project_root / "Output"
            output_dir.mkdir(exist_ok=True)
            
            output_file = output_dir / f"test_output_{self.test_session_id}_{test_file_path.stem}.xlsx"
            
            print(f"1. íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {test_file_path.name}")
            print(f"2. ì¶œë ¥ ê²½ë¡œ: {output_file}")
            
            # ë©”ì¸ í”„ë¡œì„¸ì„œ ì‹¤í–‰
            processor = MainProcessor()
            processing_start = datetime.now()
            
            result = processor.process_ledger_file(str(test_file_path), str(output_file))
            
            processing_end = datetime.now()
            processing_duration = (processing_end - processing_start).total_seconds()
            
            # ê²°ê³¼ ê²€ì¦
            print("3. ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦...")
            
            if not result['processing_successful']:
                raise Exception("ì²˜ë¦¬ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            if not Path(result['output_file']).exists():
                raise Exception("ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            performance_metrics = {
                'processing_time_seconds': processing_duration,
                'file_size_mb': test_result['file_size_mb'],
                'throughput_mb_per_sec': test_result['file_size_mb'] / processing_duration,
                'total_accounts': result['processing_stats']['total_accounts'],
                'processed_accounts': result['processing_stats']['processed_accounts'],
                'marked_cells': result['marked_cells_count'],
                'contamination_alerts': result['contamination_alerts']
            }
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê²€ì¦
            quality_score = result['quality_assessment'].get('overall_score', 0)
            quality_grade = result['quality_assessment'].get('quality_grade', 'F')
            
            print(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_duration:.1f}ì´ˆ")
            print(f"   - í’ˆì§ˆ ì ìˆ˜: {quality_score}ì  ({quality_grade})")
            print(f"   - ì²˜ë¦¬ ê³„ì •: {result['processing_stats']['processed_accounts']}/{result['processing_stats']['total_accounts']}")
            print(f"   - ë§ˆí‚¹ëœ ì…€: {result['marked_cells_count']}ê°œ")
            print(f"   - êµì°¨ ì˜¤ì—¼: {result['contamination_alerts']}ê±´")
            
            test_result['success'] = True
            test_result['details'] = {
                'processing_result': result,
                'performance_metrics': performance_metrics,
                'output_file_created': True,
                'quality_assessment': result['quality_assessment']
            }
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
            self.test_results['performance_metrics'][test_file_path.name] = performance_metrics
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            test_result['details']['error'] = str(e)
            test_result['details']['traceback'] = traceback.format_exc()
            self.test_results['error_summary'].append({
                'file': test_file_path.name,
                'error': str(e)
            })
        
        test_result['end_time'] = datetime.now().isoformat()
        test_result['duration'] = (datetime.fromisoformat(test_result['end_time']) - 
                                  datetime.fromisoformat(test_result['start_time'])).total_seconds()
        
        self.test_results['tests_performed'].append(test_result)
        return test_result['success']
    
    def run_claude_md_compliance_test(self):
        """CLAUDE.md ê·œì¹™ ì¤€ìˆ˜ í…ŒìŠ¤íŠ¸"""
        print("--- CLAUDE.md ê·œì¹™ ì¤€ìˆ˜ í…ŒìŠ¤íŠ¸ ---")
        
        test_result = {
            'test_name': 'claude_md_compliance_test',
            'start_time': datetime.now().isoformat(),
            'success': False,
            'details': {}
        }
        
        compliance_checks = {
            'utf8_encoding': False,
            'data_integrity_priority': False,
            'yellow_marking_system': False,
            'contamination_prevention': False,
            'no_estimation_rule': False
        }
        
        try:
            print("1. UTF-8 ì¸ì½”ë”© ë³´ì¥ ê²€ì¦...")
            
            # ì„ì‹œ ë¡œê±° ìƒì„±í•˜ì—¬ UTF-8 í…ŒìŠ¤íŠ¸
            temp_logger = UTF8LoggingSystem("./temp_test_log")
            
            # í•œê¸€ í…ìŠ¤íŠ¸ ë¡œê¹… í…ŒìŠ¤íŠ¸
            korean_test_message = "í•œê¸€ í…ŒìŠ¤íŠ¸: ë°ì´í„° ì™„ê²°ì„± ìµœìš°ì„  ì›ì¹™"
            temp_logger.log_validation_event('INFO', 'í…ŒìŠ¤íŠ¸ê³„ì •', 'í•œê¸€í…ŒìŠ¤íŠ¸', korean_test_message)
            
            # UTF-8 ì¸ì½”ë”© ê²€ì¦
            encoding_check = temp_logger.validate_utf8_encoding()
            compliance_checks['utf8_encoding'] = encoding_check['all_files_utf8']
            
            temp_logger.close_session()
            
            if compliance_checks['utf8_encoding']:
                print("âœ… UTF-8 ì¸ì½”ë”© ë³´ì¥")
            else:
                print("âŒ UTF-8 ì¸ì½”ë”© ì‹¤íŒ¨")
            
            print("2. ë°ì´í„° ì™„ê²°ì„± ìš°ì„  ì›ì¹™ ê²€ì¦...")
            
            # ë°ì´í„° ê²€ì¦ í”„ë ˆì„ì›Œí¬ í™•ì¸
            from validation_framework import DataValidator
            validator = DataValidator()
            
            # ë¶ˆí™•ì‹¤í•œ ë°ì´í„° ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            test_workbook_path = self.ledger_dir / "test_sample.xlsx"
            
            # DataValidatorì˜ í•µì‹¬ ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
            required_methods = [
                'extract_account_code',
                'classify_account_type',
                'validate_carry_forward',
                'detect_cross_contamination'
            ]
            
            for method in required_methods:
                if not hasattr(validator, method):
                    raise Exception(f"í•„ìˆ˜ ê²€ì¦ ë©”ì„œë“œ ëˆ„ë½: {method}")
            
            compliance_checks['data_integrity_priority'] = True
            print("âœ… ë°ì´í„° ì™„ê²°ì„± ìš°ì„  ì›ì¹™")
            
            print("3. ë…¸ë€ìƒ‰ ë§ˆí‚¹ ì‹œìŠ¤í…œ ê²€ì¦...")
            
            from marking_system import YellowMarkingSystem
            marker = YellowMarkingSystem()
            
            # ë§ˆí‚¹ ì‹œìŠ¤í…œ í•µì‹¬ ê¸°ëŠ¥ í™•ì¸
            if hasattr(marker, 'mark_uncertain_cell') and hasattr(marker, 'yellow_fill'):
                compliance_checks['yellow_marking_system'] = True
                print("âœ… ë…¸ë€ìƒ‰ ë§ˆí‚¹ ì‹œìŠ¤í…œ")
            else:
                print("âŒ ë…¸ë€ìƒ‰ ë§ˆí‚¹ ì‹œìŠ¤í…œ ë¶ˆì™„ì „")
            
            print("4. êµì°¨ ì˜¤ì—¼ ë°©ì§€ ì‹œìŠ¤í…œ ê²€ì¦...")
            
            # êµì°¨ ì˜¤ì—¼ ê°ì§€ ê¸°ëŠ¥ í™•ì¸
            if hasattr(validator, 'detect_cross_contamination'):
                compliance_checks['contamination_prevention'] = True
                print("âœ… êµì°¨ ì˜¤ì—¼ ë°©ì§€ ì‹œìŠ¤í…œ")
            else:
                print("âŒ êµì°¨ ì˜¤ì—¼ ë°©ì§€ ì‹œìŠ¤í…œ ëˆ„ë½")
            
            print("5. ì¶”ì •ê°’ ì…ë ¥ ê¸ˆì§€ ê·œì¹™ ê²€ì¦...")
            
            # ë§ˆí‚¹ ì‹œ ê°’ ë¹„ìš°ê¸° ê·œì¹™ í™•ì¸
            from openpyxl import Workbook
            temp_wb = Workbook()
            temp_sheet = temp_wb.active
            temp_sheet['A1'].value = "í…ŒìŠ¤íŠ¸ê°’"
            
            # ë§ˆí‚¹ ìˆ˜í–‰
            marker.mark_uncertain_cell(
                temp_wb, temp_sheet.title, 'A1', 'TEST', 
                'í…ŒìŠ¤íŠ¸ì´ìŠˆ', 'ì¶”ì •ê¸ˆì§€í…ŒìŠ¤íŠ¸', "ì›ë³¸ê°’"
            )
            
            # ê°’ì´ ë¹„ì›Œì¡ŒëŠ”ì§€ í™•ì¸
            if temp_sheet['A1'].value is None:
                compliance_checks['no_estimation_rule'] = True
                print("âœ… ì¶”ì •ê°’ ì…ë ¥ ê¸ˆì§€")
            else:
                print("âŒ ì¶”ì •ê°’ ì…ë ¥ ê¸ˆì§€ ë¯¸ì¤€ìˆ˜")
            
            # ì „ì²´ ì¤€ìˆ˜ ì—¬ë¶€ íŒë‹¨
            all_compliant = all(compliance_checks.values())
            
            if all_compliant:
                print("âœ… ëª¨ë“  CLAUDE.md ê·œì¹™ ì¤€ìˆ˜")
            else:
                failed_checks = [k for k, v in compliance_checks.items() if not v]
                print(f"âŒ ì¼ë¶€ ê·œì¹™ ë¯¸ì¤€ìˆ˜: {failed_checks}")
            
            test_result['success'] = all_compliant
            test_result['details'] = compliance_checks
            
        except Exception as e:
            print(f"âŒ CLAUDE.md ì¤€ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            test_result['details']['error'] = str(e)
            test_result['details']['compliance_checks'] = compliance_checks
        
        test_result['end_time'] = datetime.now().isoformat()
        self.test_results['tests_performed'].append(test_result)
        return test_result['success']
    
    def run_performance_benchmark(self, test_files):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        print("--- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ---")
        
        if not test_files:
            print("âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ì–´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
            return False
        
        # ê°€ì¥ í° íŒŒì¼ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        largest_file = max(test_files, key=lambda f: f.stat().st_size)
        file_size_mb = largest_file.stat().st_size / (1024 * 1024)
        
        print(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {largest_file.name} ({file_size_mb:.1f} MB)")
        
        benchmark_start = datetime.now()
        success = self.run_file_processing_test(largest_file)
        benchmark_end = datetime.now()
        
        if success and largest_file.name in self.test_results['performance_metrics']:
            metrics = self.test_results['performance_metrics'][largest_file.name]
            
            print(f"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
            print(f"  - íŒŒì¼ í¬ê¸°: {metrics['file_size_mb']:.1f} MB")
            print(f"  - ì²˜ë¦¬ ì‹œê°„: {metrics['processing_time_seconds']:.1f}ì´ˆ")
            print(f"  - ì²˜ë¦¬ëŸ‰: {metrics['throughput_mb_per_sec']:.2f} MB/ì´ˆ")
            print(f"  - ê³„ì •ë‹¹ í‰ê·  ì²˜ë¦¬ì‹œê°„: {metrics['processing_time_seconds']/max(metrics['total_accounts'], 1):.2f}ì´ˆ")
            
            # ì„±ëŠ¥ ê¸°ì¤€ í‰ê°€
            performance_rating = "ìš°ìˆ˜"
            if metrics['throughput_mb_per_sec'] < 0.1:
                performance_rating = "ê°œì„ í•„ìš”"
            elif metrics['throughput_mb_per_sec'] < 0.5:
                performance_rating = "ë³´í†µ"
            
            print(f"  - ì„±ëŠ¥ í‰ê°€: {performance_rating}")
            
            return True
        else:
            print("âŒ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨")
            return False
    
    def generate_final_report(self):
        """ìµœì¢… í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        print("--- ìµœì¢… í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„± ---")
        
        self.test_results['end_time'] = datetime.now().isoformat()
        self.test_results['total_duration'] = (
            datetime.fromisoformat(self.test_results['end_time']) - 
            datetime.fromisoformat(self.test_results['start_time'])
        ).total_seconds()
        
        # ì „ì²´ ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        successful_tests = sum(1 for test in self.test_results['tests_performed'] if test['success'])
        total_tests = len(self.test_results['tests_performed'])
        
        self.test_results['overall_success'] = (successful_tests == total_tests and 
                                               len(self.test_results['error_summary']) == 0)
        
        self.test_results['summary'] = {
            'total_tests': total_tests,
            'successful_tests': successful_tests,
            'failed_tests': total_tests - successful_tests,
            'success_rate': (successful_tests / total_tests * 100) if total_tests > 0 else 0
        }
        
        # ë³´ê³ ì„œ íŒŒì¼ ìƒì„±
        report_dir = self.project_root / "log"
        report_dir.mkdir(exist_ok=True)
        
        report_file = report_dir / f"system_test_report_{self.test_session_id}.json"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±: {report_file}")
            
            # ìš”ì•½ ì¶œë ¥
            print("\n=== ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
            print(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ìˆ˜: {total_tests}")
            print(f"ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}")
            print(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {total_tests - successful_tests}")
            print(f"ì„±ê³µë¥ : {self.test_results['summary']['success_rate']:.1f}%")
            print(f"ì´ ì†Œìš” ì‹œê°„: {self.test_results['total_duration']:.1f}ì´ˆ")
            
            if self.test_results['overall_success']:
                print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì˜¤ë¥˜ë¥¼ ê²€í† í•´ì£¼ì„¸ìš”.")
                
                if self.test_results['error_summary']:
                    print("\nì˜¤ë¥˜ ìš”ì•½:")
                    for error in self.test_results['error_summary']:
                        print(f"  - {error['file']}: {error['error']}")
            
            return report_file
            
        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def run_full_integration_test(self):
        """ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            # 1. í…ŒìŠ¤íŠ¸ íŒŒì¼ ë°œê²¬
            test_files = self.discover_test_files()
            
            # 2. ê¸°ë³¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
            basic_test_success = self.run_basic_system_test()
            
            if not basic_test_success:
                print("âŒ ê¸°ë³¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ì¸í•œ ì¡°ê¸° ì¢…ë£Œ")
                return False
            
            # 3. CLAUDE.md ì¤€ìˆ˜ í…ŒìŠ¤íŠ¸
            compliance_success = self.run_claude_md_compliance_test()
            
            # 4. ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (ìµœëŒ€ 3ê°œ íŒŒì¼)
            file_test_success = True
            test_file_limit = min(3, len(test_files))
            
            for i, test_file in enumerate(test_files[:test_file_limit]):
                print(f"\níŒŒì¼ í…ŒìŠ¤íŠ¸ {i+1}/{test_file_limit}")
                file_success = self.run_file_processing_test(test_file)
                if not file_success:
                    file_test_success = False
            
            # 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì„ íƒì‚¬í•­)
            if test_files:
                print(f"\nì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
                self.run_performance_benchmark(test_files)
            
            # 6. ìµœì¢… ë³´ê³ ì„œ ìƒì„±
            print(f"\nìµœì¢… ë³´ê³ ì„œ ìƒì„±")
            report_file = self.generate_final_report()
            
            return self.test_results['overall_success']
            
        except Exception as e:
            print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            print(traceback.format_exc())
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì¬ë¬´ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ - í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    integration_test = SystemIntegrationTest()
    success = integration_test.run_full_integration_test()
    
    # ì¢…ë£Œ ì½”ë“œ ì„¤ì •
    exit_code = 0 if success else 1
    
    print(f"\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ. ì¢…ë£Œ ì½”ë“œ: {exit_code}")
    
    return exit_code

if __name__ == "__main__":
    sys.exit(main())